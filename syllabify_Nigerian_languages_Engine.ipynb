{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7eee2a79",
      "metadata": {
        "id": "7eee2a79"
      },
      "source": [
        "# üìò Syllabify Nigerian Language Wordlists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "85b6e26d",
      "metadata": {
        "id": "85b6e26d"
      },
      "outputs": [],
      "source": [
        "# This function classifies a token as a vowel (V), consonant (C), or syllabic nasal (N)\n",
        "def classify_token(token, V_set, C_set, N_set):\n",
        "    if token in V_set:\n",
        "        return 'V'\n",
        "    elif token in N_set:\n",
        "        return 'N'\n",
        "    elif token in C_set:\n",
        "        return 'C'\n",
        "    else:\n",
        "        return '?'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6777473c",
      "metadata": {
        "id": "6777473c"
      },
      "outputs": [],
      "source": [
        "#Import important Python libraries\n",
        "# Import pandas for handling Excel files and dataframes\n",
        "import pandas as pd\n",
        "\n",
        "# Import pathlib for handling file paths in a platform-independent way\n",
        "from pathlib import Path\n",
        "\n",
        "# Import os for directory and file operations\n",
        "import os\n",
        "\n",
        "# Import re for regular expressions used in tokenization and pattern matching\n",
        "import re\n",
        "\n",
        "# Import unicodedata for handling Unicode characters (e.g., diacritics)\n",
        "import unicodedata\n",
        "\n",
        "# Import zipfile for creating compressed ZIP archives of output files\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a4e5095a",
      "metadata": {
        "id": "a4e5095a"
      },
      "outputs": [],
      "source": [
        "# This function extracts the inventory items (Vowels, Consonants, Nasals) from the text description\n",
        "def extract_inventory(pattern, text):\n",
        "    \"\"\"\n",
        "    Extracts items (e.g., Vowels, Consonants, Nasals) from the inventory description using regex.\n",
        "    \"\"\"\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    return [item.strip() for item in match.group(1).split(',')] if match else []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b863fd1e",
      "metadata": {
        "id": "b863fd1e"
      },
      "outputs": [],
      "source": [
        "# This function determines the syllabic structure of a given sequence of tokens\n",
        "def get_structure(tokens, V_set, C_set, N_set, at_start=False):\n",
        "    structure = \"\"\n",
        "    for idx, token in enumerate(tokens):\n",
        "        if token in V_set:\n",
        "            structure += \"V\"\n",
        "        elif token in N_set:\n",
        "            if at_start and idx == 0:\n",
        "                structure += \"N\"  # Only the first token of the whole word/segment can be syllabic\n",
        "            else:\n",
        "                structure += \"C\"  # Elsewhere, treat nasals as consonants\n",
        "        elif token in C_set:\n",
        "            structure += \"C\"\n",
        "        else:\n",
        "            structure += \"?\"  # Unknown/invalid token\n",
        "    return structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce828e2a",
      "metadata": {
        "id": "ce828e2a"
      },
      "outputs": [],
      "source": [
        "# This function normalizes the syllable structure string into a list of valid patterns\n",
        "def normalize_structures(raw_structure):\n",
        "    structures = []\n",
        "    for s in str(raw_structure).split(','):\n",
        "        s = s.strip()\n",
        "        if '(' in s and ')' in s:\n",
        "            embedded = re.findall(r'\\((.*?)\\)', s)\n",
        "            for part in embedded:\n",
        "                structures.extend([x.strip() for x in part.split(',')])\n",
        "        else:\n",
        "            structures.append(s)\n",
        "    return structures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "87cc3da0",
      "metadata": {
        "id": "87cc3da0"
      },
      "outputs": [],
      "source": [
        "# This function splits a word into tokens based on the language's speech inventory\n",
        "def tokenize(word, inventory):\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    while i < len(word):\n",
        "        matched = False\n",
        "        # Try matching up to 3-letter sequences (for complex consonants like 'kp', 'gh', 'ny', etc.)\n",
        "        for l in range(3, 0, -1):\n",
        "            if i + l <= len(word) and word[i:i+l] in inventory:\n",
        "                tokens.append(word[i:i+l])\n",
        "                i += l\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            tokens.append(word[i])\n",
        "            i += 1\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7b25ee5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "7b25ee5d",
        "outputId": "232d50ab-bb85-4034-bc73-5cf8cdeb77a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8efd2cfb-b3d1-4c3c-98ab-0bf026bf451a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8efd2cfb-b3d1-4c3c-98ab-0bf026bf451a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Language_Profile_Wordlist.xlsx to Language_Profile_Wordlist.xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload Language_Profile_Wordlist.xlsx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c0b24fba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b24fba",
        "outputId": "0b20bd99-df31-4fd5-f250-1f358d579dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/172.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1300c84f",
      "metadata": {
        "id": "1300c84f"
      },
      "outputs": [],
      "source": [
        "#   This function inserts hyphens in words with repeated (identical) consonants:\n",
        "#   - If allows_clusters=True, insert hyphen BEFORE the first identical consonant (e.g., dikke ‚Üí di-kke)\n",
        "#   - If allows_clusters=False, insert hyphen AFTER the first identical consonant (e.g., dikke ‚Üí dik-ke)\n",
        "\n",
        "def preprocess_double_consonants(word, consonants, allows_clusters=True):\n",
        "    result = []\n",
        "    i = 0\n",
        "    while i < len(word) - 1:\n",
        "        current = word[i]\n",
        "        next_char = word[i + 1]\n",
        "\n",
        "        if current == next_char and current in consonants:\n",
        "            if allows_clusters:\n",
        "                result.extend([word[:i], '-', word[i:]])\n",
        "            else:\n",
        "                result.extend([word[:i+1], '-', word[i+1:]])\n",
        "            return ''.join(result)\n",
        "        i += 1\n",
        "    return word\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect and hyphenate repeated substrings of ‚â•3 characters\n",
        "def insert_repetition_hyphenation(word):\n",
        "    for length in range(6, 2, -1):  # Check for substrings of length 6 to 3\n",
        "        for i in range(len(word) - 2 * length + 1):\n",
        "            first = word[i:i+length]\n",
        "            second = word[i+length:i+2*length]\n",
        "            if first == second:\n",
        "                return word[:i] + first + '-' + word[i+length:]\n",
        "    return word"
      ],
      "metadata": {
        "id": "d3KEwo-3QcmW"
      },
      "id": "d3KEwo-3QcmW",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "be3588d0",
      "metadata": {
        "id": "be3588d0"
      },
      "outputs": [],
      "source": [
        "# This function syllabifies a single word using the language-specific phonotactic rules\n",
        "def syllabify_word(word, V_set, C_set, N_set, structures):\n",
        "    segments = re.split(r'[\\s\\-]', word)\n",
        "    syllabified_segments = []\n",
        "\n",
        "    for segment in segments:\n",
        "        tokens = tokenize(segment, V_set | C_set | N_set)\n",
        "        result = []\n",
        "        i = 0\n",
        "        rule1_applied = False\n",
        "\n",
        "        # Rule 1: if first is V or N and followed by C/N, insert hyphen\n",
        "        if len(tokens) >= 2:\n",
        "            first = classify_token(tokens[0], V_set, C_set, N_set)\n",
        "            if first in {'V', 'N'} and tokens[1] in (C_set | N_set):\n",
        "                result.append(tokens[0])\n",
        "                result.append('-')\n",
        "                tokens = tokens[1:]\n",
        "                i = 0\n",
        "                rule1_applied = True\n",
        "        # After Rule 1, treat all nasals as consonants\n",
        "        full_C_set = C_set | N_set\n",
        "\n",
        "        while i < len(tokens):\n",
        "            applied = False\n",
        "            remaining = tokens[i:]\n",
        "\n",
        "            if not rule1_applied and get_structure(remaining, V_set, full_C_set, set()) in structures:\n",
        "                result.extend(remaining)\n",
        "                break\n",
        "\n",
        "            for j in range(min(5, len(remaining)), 0, -1):\n",
        "                chunk = remaining[:j]\n",
        "                next_index = i + j\n",
        "                if get_structure(chunk, V_set, full_C_set, set()) in structures:\n",
        "                    # Only apply if followed by consonant or end\n",
        "                    if next_index < len(tokens):\n",
        "                        if classify_token(tokens[next_index], V_set, full_C_set, set()) == 'C':\n",
        "                            result.extend(chunk + ['-'])\n",
        "                            i = next_index\n",
        "                            applied = True\n",
        "                            break\n",
        "                    else:\n",
        "                        result.extend(chunk)\n",
        "                        i = next_index\n",
        "                        applied = True\n",
        "                        break\n",
        "\n",
        "            if not applied:\n",
        "                result.append(tokens[i])\n",
        "                i += 1\n",
        "\n",
        "        syllabified_segments.append(''.join(result).strip('-'))\n",
        "\n",
        "    return '-'.join(syllabified_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "25fdbaa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25fdbaa9",
        "outputId": "79c0ac0e-b40d-4654-9c16-0c0126c24b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All languages processed. Output saved to: syllabified_output\n"
          ]
        }
      ],
      "source": [
        "# This function processes all language sheets in the Excel file and syllabifies the transcribed words\n",
        "def syllabify_all_languages(input_excel_path, output_dir_path):\n",
        "    xls = pd.ExcelFile(input_excel_path)\n",
        "    profile_df = xls.parse(\"Language_Profile\")\n",
        "    profile_rows = profile_df.set_index(\"SN\").to_dict(orient=\"index\")\n",
        "\n",
        "    Path(output_dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for sn, sheet_name in enumerate(xls.sheet_names[1:], start=1):  # Skip Language_Profile\n",
        "        profile_row = profile_rows.get(sn)\n",
        "        if not profile_row:\n",
        "            continue\n",
        "\n",
        "        df = xls.parse(sheet_name)\n",
        "        if not {\"English word\", \"Transcribed word\", \"Tone pattern\"}.issubset(df.columns):\n",
        "            continue\n",
        "\n",
        "        inventory_text = profile_row[\"Speech and sound inventory\"]\n",
        "        structures = normalize_structures(profile_row[\"Syllable structure\"])\n",
        "        V = extract_inventory(r'Vowels?\\s*\\(V\\):\\s*([^\\n\\r\\.]+)', inventory_text)\n",
        "        C = extract_inventory(r'Consonants?\\s*\\(C\\):\\s*([^\\n\\r\\.]+)', inventory_text)\n",
        "        N = extract_inventory(r'Syllabic nasals?\\s*\\(N\\):\\s*([^\\n\\r\\.]+)', inventory_text)\n",
        "\n",
        "        V_set, C_set, N_set = set(V), set(C), set(N)\n",
        "\n",
        "        df[\"Syllabified word\"] = df[\"Transcribed word\"].astype(str).apply(\n",
        "            lambda word: syllabify_word(insert_repetition_hyphenation(word), V_set, C_set | N_set, N_set, structures)\n",
        "        )\n",
        "\n",
        "        def classify_segment(segment, V, C, N, full_word, segment_start_index):\n",
        "            structure = []\n",
        "            i = 0\n",
        "            while i < len(segment):\n",
        "                matched = False\n",
        "                for unit in sorted(V + C + N, key=lambda x: -len(x)):\n",
        "                    if segment[i:].startswith(unit):\n",
        "                        absolute_index = segment_start_index + i\n",
        "                        next_char = full_word[absolute_index + len(unit):absolute_index + len(unit) + 1]\n",
        "                        prev_char = full_word[absolute_index - 1] if absolute_index > 0 else ''\n",
        "\n",
        "                        if unit in N:\n",
        "                            is_initial = (absolute_index == 0 or prev_char in {'-', ' '})\n",
        "                            if is_initial and next_char and any(next_char.startswith(c) for c in C):\n",
        "                                structure.append('N')\n",
        "                            else:\n",
        "                                structure.append('C')\n",
        "                        elif unit in V:\n",
        "                            structure.append('V')\n",
        "                        elif unit in C:\n",
        "                            structure.append('C')\n",
        "                        else:\n",
        "                            structure.append('?')\n",
        "                        i += len(unit)\n",
        "                        matched = True\n",
        "                        break\n",
        "                if not matched:\n",
        "                    structure.append('?')\n",
        "                    i += 1\n",
        "            return ''.join(structure)\n",
        "\n",
        "        def generate_ncv_structure(row):\n",
        "            transcribed = row[\"Transcribed word\"]\n",
        "            syllabified = row[\"Syllabified word\"]\n",
        "            if not isinstance(syllabified, str) or not syllabified.strip():\n",
        "                return \"\"\n",
        "            segments = syllabified.split('-')\n",
        "            cv_parts = []\n",
        "            idx = 0\n",
        "            for seg in segments:\n",
        "                while idx < len(transcribed) and transcribed[idx] in {'-', ' '}:\n",
        "                    idx += 1\n",
        "                cv = classify_segment(seg, V, C, N, transcribed, idx)\n",
        "                cv_parts.append(cv)\n",
        "                idx += len(seg)\n",
        "            return '-'.join(cv_parts)\n",
        "\n",
        "        df[\"NCV_Structure\"] = df.apply(generate_ncv_structure, axis=1)\n",
        "\n",
        "        def validate_characters(word, inventory_set):\n",
        "            tokens = tokenize(word, inventory_set)\n",
        "            tokens = [t for t in tokens if t not in {'-', ' '}]\n",
        "            unrecognized = [t for t in tokens if t not in inventory_set]\n",
        "            if unrecognized:\n",
        "                return f\"wrong syllabification detected: character(s) {', '.join(unrecognized)} not in the speech and sound inventory of this language\"\n",
        "            else:\n",
        "                return \"transcribed word correctly syllabified using defined syllable structure of this language\"\n",
        "\n",
        "        df[\"Syllabification Validation\"] = df[\"Transcribed word\"].astype(str).apply(\n",
        "            lambda word: validate_characters(word, V_set | C_set | N_set)\n",
        "        )\n",
        "\n",
        "        lang_name = profile_row[\"Language\"]\n",
        "        output_file = os.path.join(output_dir_path, f\"{lang_name}.xlsx\")\n",
        "        cv_output_dir = \"structure_transformed_files\"\n",
        "        os.makedirs(cv_output_dir, exist_ok=True)\n",
        "        cv_output_path = os.path.join(cv_output_dir, f\"{lang_name}.xlsx\")\n",
        "        df.to_excel(cv_output_path, index=False)\n",
        "\n",
        "        with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
        "            df.drop(columns=[\"NCV_Structure\"], errors=\"ignore\").to_excel(writer, sheet_name=lang_name, index=False)\n",
        "\n",
        "    print(f\"‚úì All languages processed. Output saved to: {output_dir_path}\")\n",
        "\n",
        "# Call the function\n",
        "syllabify_all_languages(\"Language_Profile_Wordlist.xlsx\", \"syllabified_output\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9714b07",
      "metadata": {
        "id": "a9714b07"
      },
      "source": [
        "## üì¶ Download All Results as ZIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "84c58e36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84c58e36",
        "outputId": "8ba15cd3-1cda-4579-a852-936c88ab2e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All syllabified files zipped at: syllabified_output.zip\n",
            "‚úÖ All structure-transformed files zipped at: structure_transformed_files.zip\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Zip all syllabified output files\n",
        "output_zip_path = \"syllabified_output.zip\"\n",
        "with zipfile.ZipFile(output_zip_path, 'w') as zipf:\n",
        "    for root, _, files in os.walk(\"syllabified_output\"):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, \"syllabified_output\")\n",
        "            zipf.write(file_path, arcname)\n",
        "print(f\"‚úÖ All syllabified files zipped at: {output_zip_path}\")\n",
        "\n",
        "# üì¶ Zip all structure-transformed output files\n",
        "structure_zip_path = \"structure_transformed_files.zip\"\n",
        "with zipfile.ZipFile(structure_zip_path, 'w') as zipf:\n",
        "    for root, _, files in os.walk(\"structure_transformed_files\"):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, \"structure_transformed_files\")\n",
        "            zipf.write(file_path, arcname)\n",
        "print(f\"‚úÖ All structure-transformed files zipped at: {structure_zip_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}